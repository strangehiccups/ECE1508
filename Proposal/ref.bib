@inproceedings{lin2014microsoft,
	title={Microsoft {COCO}: Common objects in context},
	author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
	booktitle={Proc. 13th European Conference on Computer Vision (ECCV)},
	pages={740--755},
	year={2014},
	organization={Springer},
	address={Z\"urich, Switzerland}
}

@inproceedings{radford2021learning,
	title={Learning transferable visual models from natural language supervision},
	author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages={8748--8763},
	year={2021},
	organization={PMLR}
}

@article{hodosh2013framing,
	title={Framing image description as a ranking task: Data, models and evaluation metrics},
	author={Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
	journal={Journal of Artificial Intelligence Research},
	volume={47},
	pages={853--899},
	year={2013}
}

@inproceedings{vedantam2015cider,
	title={{CIDEr}: Consensus-based image description evaluation},
	author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
	booktitle={Proc. IEEE Conference on Computer Vision and Pattern Recognition},
	pages={4566--4575},
	year={2015}
}

@article{dallE2021openAI,
	title={DALL-E: Creating images from text},
	author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott},
	journal={OpenAI Blog: Milestone},
	pages={available at \texttt{https://openai.com/index/dall-e/}},
	year={2021}
}

@inproceedings{devlin2019bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle={Proc. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	pages={4171--4186},
	year={2019}
}

@article{liu2019roberta,
	title={Roberta: A robustly optimized bert pretraining approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}

@article{sanh2019distilbert,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
	author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	journal={arXiv preprint arXiv:1910.01108},
	year={2019}
}

@article{dosovitskiy2020image,
	title={An image is worth 16x16 words: Transformers for image recognition at scale},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	journal={arXiv preprint arXiv:2010.11929},
	year={2020}
}

@inproceedings{chen2020simple,
	title={A simple framework for contrastive learning of visual representations},
	author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages={1597--1607},
	year={2020},
	organization={PMLR}
}


@article{oord2018representation,
	title={Representation learning with contrastive predictive coding},
	author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	journal={arXiv preprint arXiv:1807.03748},
	year={2018}
}


